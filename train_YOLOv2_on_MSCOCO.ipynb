{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train YOLOv2 on MSCOCO Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "- 基本的にはFaster R-CNNのRPNの精度を高めたもの\n",
    "- 入力を(224, 224)の画像とすると、出力されるfeatureは(7, 7)\n",
    "- featureの１マスが入力画像の(32, 32)の領域に対応しているとみなす\n",
    "- featureの各マスについて、\n",
    "  - そこに物体がある確率(confidence)\n",
    "  - 物体があるとすればcategoryは何か\n",
    "  - そこに物体がある場合の矩形の中心(x, y), 大きさ(w, h)\n",
    "    - (x, y)はマスの中心を(0.5, 0.5)として[0, 1]の値\n",
    "    - (w, h)はanchor box(後述)の何倍であるか\n",
    "- anchor boxは基準となる矩形であり、これとground truthのIoUが高いときそこには物体がある(confidence=1)と考える\n",
    "- 詳しいことは[この辺り](https://github.com/leetenki/YOLOv2/blob/master/YOLOv2.md)を読んで下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare YOLOv2\n",
    "- YOLOv2と学習に必要な関数の用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calc Intersection over Union of two bounding boxes.\n",
    "def calc_iou(box0, box1, xp):\n",
    "    # box: (x, y, w, h)\n",
    "    x0 = xp.maximum(\n",
    "        (box0[0] - box0[2] / 2), (box1[0] - box1[2] / 2))\n",
    "    y0 = xp.maximum(\n",
    "        (box0[1] - box0[3] / 2), (box1[1] - box1[3] / 2))\n",
    "    x1 = xp.minimum(\n",
    "        (box0[0] + box0[2] / 2), (box1[0] + box1[2] / 2))\n",
    "    y1 = xp.minimum(\n",
    "        (box0[1] + box0[3] / 2), (box1[1] + box1[3] / 2))\n",
    "    mask = (x1 > x0) * (y1 > y0)\n",
    "    intersection = (x1 - x0) * (y1 - y0) * mask\n",
    "    sum_box = box0[2] * box0[3] + box1[2] * box1[3] - intersection\n",
    "    return xp.array(intersection / sum_box)\n",
    "\n",
    "# make high resolution feature.\n",
    "def reorg(x, stride=2):\n",
    "    b, ch, h, w = x.shape\n",
    "    out_h, out_w, out_ch = h // stride, w // stride, ch * (stride**2)\n",
    "    out = F.reshape(x, (b, ch, out_h, stride, out_w, stride))\n",
    "    out = F.transpose(out, (0, 1, 3, 5, 2, 4))\n",
    "    out = F.reshape(out, (b, out_ch, out_h, out_w))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvBlock(chainer.Chain):\n",
    "    def __init__(self, in_ch, out_ch, ksize, use_bn=True, activation=True):\n",
    "        self.use_bn = use_bn\n",
    "        self.activation = activation\n",
    "        pad = (ksize - 1) // 2\n",
    "        layers = {}\n",
    "        layers['conv'] = L.Convolution2D(in_ch, out_ch, ksize=ksize,\n",
    "                                         stride=1, pad=pad)\n",
    "        if use_bn:\n",
    "            layers['bn'] = L.BatchNormalization(out_ch)\n",
    "        super(ConvBlock, self).__init__(**layers)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.use_bn:\n",
    "            h = self.bn(h)\n",
    "        if self.activation:\n",
    "            h = F.leaky_relu(h, slope=0.1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YOLOv2(chainer.Chain):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.thresh = 0.6\n",
    "        self.anchors = [[5.375,   5.03125],\n",
    "                        [5.40625,  4.6875],\n",
    "                        [2.96875, 2.53125],\n",
    "                        [2.59375, 2.78125],\n",
    "                        [1.9375,     3.25]]\n",
    "        self.n_anchors = len(self.anchors)\n",
    "        # learning scale\n",
    "        self.lambda_box_ls_obj    = 1.0\n",
    "        self.lambda_box_ls_noobj  = 0.1\n",
    "        self.lambda_conf_ls_obj   = 10.0\n",
    "        self.lambda_conf_ls_noobj = 0.1\n",
    "\n",
    "        super(YOLOv2, self).__init__(\n",
    "            cb1  = ConvBlock(   3,   32, 3),\n",
    "            cb2  = ConvBlock(  32,   64, 3),\n",
    "            cb3  = ConvBlock(  64,  128, 3),\n",
    "            cb4  = ConvBlock( 128,   64, 1),\n",
    "            cb5  = ConvBlock(  64,  128, 3),\n",
    "            cb6  = ConvBlock( 128,  256, 3),\n",
    "            cb7  = ConvBlock( 256,  128, 1),\n",
    "            cb8  = ConvBlock( 128,  256, 3),\n",
    "            cb9  = ConvBlock( 256,  512, 3),\n",
    "            cb10 = ConvBlock( 512,  256, 1),\n",
    "            cb11 = ConvBlock( 256,  512, 3),\n",
    "            cb12 = ConvBlock( 512,  256, 1),\n",
    "            cb13 = ConvBlock( 256,  512, 3),\n",
    "            cb14 = ConvBlock( 512, 1024, 3),\n",
    "            cb15 = ConvBlock(1024,  512, 1),\n",
    "            cb16 = ConvBlock( 512, 1024, 3),\n",
    "            cb17 = ConvBlock(1024,  512, 1),\n",
    "            cb18 = ConvBlock( 512, 1024, 3),\n",
    "            cb19 = ConvBlock(1024, 1024, 3),\n",
    "            cb20 = ConvBlock(1024, 1024, 3),\n",
    "            cb21 = ConvBlock(3072, 1024, 3),\n",
    "            cb22 = ConvBlock(1024, self.n_anchors * (5 + n_classes),\n",
    "                           1, use_bn=False, activation=False),\n",
    "        )\n",
    "\n",
    "    def calc_feat(self, x):\n",
    "        h = x\n",
    "        for i in range(1, 23):\n",
    "            h = self['cb{}'.format(i)](h)\n",
    "            if i == 13:\n",
    "                hires_feat = reorg(h)\n",
    "            if i in [1, 2, 5, 8, 13]:\n",
    "                h = F.max_pooling_2d(h, ksize=2, stride=2, pad=0)\n",
    "            if i == 20:\n",
    "                h = F.concat((hires_feat, h), axis=1)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.calc_feat(x)\n",
    "        b, ch, grid_h, grid_w = feat.shape\n",
    "        feat = F.reshape(\n",
    "            feat, (b, self.n_anchors, self.n_classes + 5, grid_h, grid_w))\n",
    "        x, y, w, h, conf, cls = F.split_axis(feat, (1, 2, 3, 4, 5), axis=2)\n",
    "        x = F.sigmoid(x)\n",
    "        y = F.sigmoid(y)\n",
    "        conf = F.sigmoid(conf)\n",
    "        cls = F.transpose(cls, (0, 2, 1, 3, 4))\n",
    "        cls = F.softmax(cls)\n",
    "        return x, y, w, h, conf, cls, (b, ch, grid_h, grid_w)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        chainer.config.train = True\n",
    "        # forward\n",
    "        x, y, w, h, conf, cls, (batch, ch, grid_h, grid_w) = self.forward(x)\n",
    "\n",
    "        # make target\n",
    "        # TODO: めちゃ遅いのでなんとかする\n",
    "        anchors = self.xp.array(self.anchors, dtype=np.float32)\n",
    "\n",
    "        # prepare target array\n",
    "        t_w = self.xp.zeros_like(w)\n",
    "        t_h = self.xp.zeros_like(h)\n",
    "        t_x = self.xp.zeros_like(x) + 0.5\n",
    "        t_y = self.xp.zeros_like(y) + 0.5\n",
    "        t_conf = self.xp.zeros_like(conf)\n",
    "        t_cls = cls.data.copy()\n",
    "\n",
    "        # learning scales\n",
    "        box_ls = self.xp.zeros_like(x) + self.lambda_box_ls_noobj # 0.1\n",
    "        conf_ls = self.xp.zeros_like(conf) + self.lambda_conf_ls_noobj\n",
    "\n",
    "        # to absolute coordinates\n",
    "        w_anchors_abs = anchors[:, 1, None, None, None] / grid_w\n",
    "        h_anchors_abs = anchors[:, 0, None, None, None] / grid_h\n",
    "        x_shift, y_shift = self.xp.meshgrid(\n",
    "            self.xp.linspace(0, grid_w - 1, grid_w, dtype=np.float32),\n",
    "            self.xp.linspace(0, grid_h - 1, grid_h, dtype=np.float32))\n",
    "        x_abs = (x + x_shift) / grid_w\n",
    "        y_abs = (y + y_shift) / grid_h\n",
    "        w_abs = F.exp(w) * w_anchors_abs\n",
    "        h_abs = F.exp(h) * h_anchors_abs\n",
    "\n",
    "        for b in range(batch):\n",
    "            for t_box in t[b]:\n",
    "                # prepare t_pos and t_cls\n",
    "                ind_w = int(t_box['x'] * grid_w)\n",
    "                ind_h = int(t_box['y'] * grid_h)\n",
    "                ious = calc_iou((0, 0, t_box['w'], t_box['h']),\n",
    "                                (0, 0, w_anchors_abs, h_anchors_abs), self.xp)\n",
    "                ind_anchor = self.xp.argmax(ious)\n",
    "                box_ls[b, ind_anchor, :, ind_h, ind_w] = self.lambda_box_ls_obj\n",
    "                t_x[b, ind_anchor, :, ind_h, ind_w] = t_box['x'] * grid_w - ind_w\n",
    "                t_y[b, ind_anchor, :, ind_h, ind_w] = t_box['y'] * grid_h - ind_h\n",
    "                t_w[b, ind_anchor, :, ind_h, ind_w] = self.xp.log(\n",
    "                    t_box['w'] / w_anchors_abs[ind_anchor, 0, 0])\n",
    "                t_h[b, ind_anchor, :, ind_h, ind_w] = self.xp.log(\n",
    "                    t_box['h'] / h_anchors_abs[ind_anchor, 0, 0])\n",
    "                t_cls[b, :, ind_anchor, ind_h, ind_w] = 0.\n",
    "                t_cls[b, int(t_box['label']), ind_anchor, ind_h, ind_w] = 1.\n",
    "\n",
    "                # prepare t_conf\n",
    "                iou_pred = calc_iou((t_box['x'], t_box['y'], t_box['w'], t_box['h']),\n",
    "                                    (x_abs[b].data,\n",
    "                                     y_abs[b].data,\n",
    "                                     w_abs[b].data,\n",
    "                                     h_abs[b].data),\n",
    "                                    self.xp)\n",
    "                t_conf[b, ind_anchor, :, ind_h, ind_w] = iou_pred[ind_anchor, :, ind_h, ind_w]\n",
    "                conf_ls[b][(iou_pred > self.thresh) * \n",
    "                           (conf_ls[b] != self.lambda_conf_ls_obj)] = 0.\n",
    "                conf_ls[b, ind_anchor, :, ind_h, ind_w] = self.lambda_conf_ls_obj\n",
    "\n",
    "            # calc Loss\n",
    "            loss_x = F.sum((t_x - x) ** 2 * box_ls) / 2\n",
    "            loss_y = F.sum((t_y - y) ** 2 * box_ls) / 2\n",
    "            loss_w = F.sum((t_w - w) ** 2 * box_ls) / 2\n",
    "            loss_h = F.sum((t_h - h) ** 2 * box_ls) / 2\n",
    "            loss_conf = F.sum((t_conf - conf) ** 2 * conf_ls) / 2\n",
    "            loss_cls = F.sum((t_cls - cls) ** 2) / 2\n",
    "            self.loss = (loss_x + loss_y + loss_w + loss_h + \n",
    "                         loss_conf + loss_cls) / batch\n",
    "            chainer.report({'loss': self.loss}, self)\n",
    "        return self.loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        chainer.config.train = False\n",
    "        # forward\n",
    "        x, y, w, h, conf, cls, (batch, ch, grid_h, grid_w) = self.forward(x)\n",
    "        # to absolute coordinates\n",
    "        anchors = self.xp.array(self.anchors, dtype=np.float32)\n",
    "        w_anchors_abs = anchors[:, 1, None, None, None] / grid_w\n",
    "        h_anchors_abs = anchors[:, 0, None, None, None] / grid_h\n",
    "        x_shift, y_shift = self.xp.meshgrid(\n",
    "            self.xp.linspace(0, grid_w - 1, grid_w, dtype=np.float32),\n",
    "            self.xp.linspace(0, grid_h - 1, grid_h, dtype=np.float32))\n",
    "        x_abs = (x + x_shift) / grid_w\n",
    "        y_abs = (y + y_shift) / grid_h\n",
    "        w_abs = F.exp(w) * w_anchors_abs\n",
    "        h_abs = F.exp(h) * h_anchors_abs\n",
    "        return x_abs, y_abs, w_abs, h_abs, conf, cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## prepare MSCOCO Dataset\n",
    "### download dataset\n",
    "- 上から実行していくとMSCOCOデータセットがダウンロードされ使えるようになります(たぶん)\n",
    "- それなりの空き容量が必要です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download MSCOCO dataset\n",
    "!wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
    "!wget http://msvocds.blob.core.windows.net/coco2014/val2014.zip\n",
    "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/instances_train-val2014.zip\n",
    "\n",
    "# unzip dataset\n",
    "!unzip ./train2014.zip\n",
    "!unzip ./val2014.zip\n",
    "!unzip ./instances_train-val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./annotations/instances_train2014.json') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ひとまず画像を見てみる\n",
    "\n",
    "ind = 1000\n",
    "image_id = annotations['annotations'][ind]['image_id']\n",
    "bbox = annotations['annotations'][ind]['bbox']\n",
    "annotations['annotations'][ind]['category_id']\n",
    "\n",
    "img = Image.open(glob.glob('./train2014/*' + str(image_id) + '*.jpg')[0])\n",
    "img = np.array(img, dtype=np.float32)\n",
    "bbox = list(map(int, bbox))\n",
    "img = cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (120, 0, 0), 3)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image pathとannotationを対応付ける\n",
    "def load_data(annotation_file):\n",
    "    with open(annotation_file) as f:\n",
    "        annotations = json.load(f)\n",
    "    data = {}\n",
    "    for img in annotations['images']:\n",
    "        data[img['id']] = {'file_name':img['file_name'], 'annos':[]}\n",
    "    for anno in annotations['annotations']:\n",
    "        img_id = anno['image_id']\n",
    "        cat_id = anno['category_id']\n",
    "        bbox   = np.array(anno['bbox'], dtype=np.float32) #lefttop_x, y, w, h\n",
    "        data[img_id]['annos'] += [(bbox, cat_id)]\n",
    "    data = list(data.values())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 画像読み込み\n",
    "def _read_img_as_array(img_path, annotation, size=(224, 224), augmentation=True):\n",
    "    img = Image.open(img_path)\n",
    "    w_img, h_img = img.size\n",
    "    annos = copy.deepcopy(annotation)\n",
    "    aug_flag = augmentation and np.random.random() > 0.5\n",
    "    # set resize parameters\n",
    "    if aug_flag:\n",
    "        if h_img > w_img:\n",
    "            w_resize = np.random.randint(size[0], size[0] * 1.2)\n",
    "            h_resize = np.random.normal(h_img * (w_resize / w_img), w_resize * 0.1)\n",
    "            h_resize = int(np.max([h_resize, size[1]]))\n",
    "        else:\n",
    "            h_resize = np.random.randint(size[1], size[1] * 1.2)\n",
    "            w_resize = np.random.normal(w_img * (h_resize / h_img), h_resize * 0.1)\n",
    "            w_resize = int(np.max([w_resize, size[0] + 1]))\n",
    "    else:\n",
    "        if h_img > w_img:\n",
    "            w_resize = size[0]\n",
    "            h_resize = int(np.ceil(h_img * w_resize / w_img))\n",
    "        else:\n",
    "            h_resize = size[1]\n",
    "            w_resize = int(np.ceil(w_img * h_resize / h_img))\n",
    "    for anno in annos:\n",
    "        box, cls = anno\n",
    "        box[[0, 2]] *= w_resize / w_img\n",
    "        box[[1, 3]] *= h_resize / h_img\n",
    "    img = np.array(img.resize((w_resize, h_resize)), dtype=np.float32)\n",
    "\n",
    "    # crop\n",
    "    offset_x = np.random.randint(0, w_resize - size[0] + 1)\n",
    "    offset_y = np.random.randint(0, h_resize - size[1] + 1)\n",
    "    img = img[offset_y:offset_y + size[1], offset_x:offset_x + size[0]]\n",
    "    img /= 255.\n",
    "    \n",
    "    if aug_flag:\n",
    "        # randomly shift gamma\n",
    "        random_gamma = np.random.uniform(0.8, 1.2)\n",
    "        img = img ** random_gamma\n",
    "        # randomly shift brightness\n",
    "        random_brightness = np.random.uniform(0.5, 2.0)\n",
    "        img = img * random_brightness\n",
    "        # randomly shift color\n",
    "        random_colors = np.random.uniform(0.8, 1.2, 3)\n",
    "        img *= random_colors\n",
    "        # saturate\n",
    "        img = np.clip(img,  0., 1.)\n",
    "    \n",
    "    t = []\n",
    "    for anno in annos:\n",
    "        box, cls = anno\n",
    "        x_tl = max(0, box[0] - offset_x)\n",
    "        y_tl = max(0, box[1] - offset_y)\n",
    "        x_br = min(x_tl + box[2], size[0])\n",
    "        y_br = min(y_tl + box[3], size[1])\n",
    "        x = ((x_tl + x_br) / 2) / size[0]\n",
    "        y = ((y_tl + y_br) / 2) / size[1]\n",
    "        w = (x_br - x_tl) / size[0]\n",
    "        h = (y_br - y_tl) / size[0]\n",
    "        if w > 0 and h > 0:\n",
    "            t.append({'x': x, 'y': y, 'w': w, 'h': h, 'label': int(cls)})\n",
    "    return img, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: img_sizeをランダムに変更できるようにする\n",
    "class Dataset(chainer.dataset.dataset_mixin.DatasetMixin):\n",
    "    def __init__(self, annotations, root='./', img_size=(224, 224),\n",
    "                 augmentation=True):\n",
    "        self.data = load_data(annotations)\n",
    "        self._root = root\n",
    "        self.augmentation = augmentation\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        annos = self.data[i]['annos']\n",
    "        file_name = self.data[i]['file_name']\n",
    "        file_path = os.path.join(self._root, file_name)\n",
    "        try:\n",
    "            x, t = _read_img_as_array(file_path, annos, size=self.img_size,\n",
    "                                      augmentation=self.augmentation)\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        except:\n",
    "            x = np.zeros((3, self.img_size[1], self.img_size[0]), dtype=np.float32)\n",
    "            t = []\n",
    "            print('load error.', file_name)\n",
    "        return x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradientにnoiseを加える。\n",
    "# 学習が安定する、気がする（AdamでもMomentum SGD程度の精度が出るようになるような（要検証））。\n",
    "def noise_proportional_to_gradient(xp, g, hook):\n",
    "    shape = g.shape\n",
    "    dtype = g.dtype\n",
    "    std = g.std()\n",
    "    return xp.random.normal(0, std * hook.gamma, shape).astype(dtype)\n",
    "\n",
    "\n",
    "class GradientNoise(object):\n",
    "    name = 'GradientNoise'\n",
    "    call_for_each_param = True\n",
    "\n",
    "    def __init__(self, gamma=0.01, noise_func=noise_proportional_to_gradient):\n",
    "        self.gamma = gamma\n",
    "        self.noise_func = noise_func\n",
    "\n",
    "    def __call__(self, rule, param):\n",
    "        g = param.grad\n",
    "        xp = cuda.get_array_module(g)\n",
    "        with cuda.get_device_from_array(g) as dev:\n",
    "            noise = self.noise_func(xp, g, self)\n",
    "            if int(dev) == -1:\n",
    "                g += noise\n",
    "            else:\n",
    "                kernel = cuda.elementwise(\n",
    "                    'T noise', 'T g', 'g += noise', 'gradient_noise')\n",
    "                kernel(noise, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batchのoriginalのままだと失敗するので書き換える\n",
    "import numpy\n",
    "import six\n",
    "\n",
    "from chainer import cuda\n",
    "\n",
    "\n",
    "def to_device(device, x):\n",
    "    if device is None:\n",
    "        return x\n",
    "    elif device < 0:\n",
    "        return cuda.to_cpu(x)\n",
    "    else:\n",
    "        return cuda.to_gpu(x, device)\n",
    "\n",
    "\n",
    "def concat_examples(batch, device=None):\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError('batch is empty')\n",
    "    result = []\n",
    "    result.append(to_device(device, _concat_arrays(\n",
    "        [example[0] for example in batch])))\n",
    "    result.append([example[1] for example in batch])\n",
    "    return tuple(result)\n",
    "\n",
    "\n",
    "def _concat_arrays(arrays):\n",
    "    if not isinstance(arrays[0], numpy.ndarray) and not isinstance(arrays[0], cuda.ndarray):\n",
    "        arrays = numpy.asarray(arrays)\n",
    "    xp = cuda.get_array_module(arrays[0])\n",
    "    with cuda.get_device_from_array(arrays[0]):\n",
    "        return xp.concatenate([array[None] for array in arrays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = YOLOv2(90)\n",
    "optimizer = chainer.optimizers.Adam(1e-4)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(1e-5))\n",
    "optimizer.add_hook(GradientNoise(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chainer import training\n",
    "from chainer import iterators\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 設定\n",
    "epoch = 50\n",
    "out_dir = './result2/'\n",
    "batchsize = 6\n",
    "\n",
    "snapshot_interval = (5000, 'iteration')\n",
    "report_interval = (10, 'iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset('../../detection/annotations/instances_train2014.json',\n",
    "                  root='../../detection/train2014/', augmentation=True, img_size=(448, 448))\n",
    "#train_iter = iterators.SerialIterator(dataset, batchsize, repeat=True, shuffle=True)\n",
    "train_iter = iterators.MultiprocessIterator(dataset, batchsize, repeat=True, shuffle=True, n_processes=2)\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=0, converter=concat_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out=out_dir)\n",
    "\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "trainer.extend(extensions.snapshot(), trigger=snapshot_interval)\n",
    "trainer.extend(extensions.snapshot_object(\n",
    "    model, 'model_iter_{.updater.iteration}'), trigger=snapshot_interval)\n",
    "trainer.extend(extensions.LogReport(trigger=report_interval))\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/loss']),\n",
    "               trigger=report_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load snapshot\n",
    "chainer.serializers.load_npz('./result2/snapshot_iter_40000', trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset('./annotations/instances_val2014.json',\n",
    "                        root='./val2014/', augmentation=False)\n",
    "test_iter = iterators.SerialIterator(dataset, 1, repeat=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_rectangls(img, recs):\n",
    "    _img = img.copy()\n",
    "    for r in recs:\n",
    "        x0 = max(0, int((r['x'] - r['w'] / 2) * img.shape[1]))\n",
    "        x1 = min(int((r['x'] + r['w'] / 2) * img.shape[1]), img.shape[1])\n",
    "        y0 = max(0, int((r['y'] - r['h'] / 2) * img.shape[0]))\n",
    "        y1 = min(int((r['y'] + r['h'] / 2) * img.shape[0]), img.shape[0])\n",
    "        _img = cv2.rectangle(_img, (x0, y0), (x1, y1), (120, 0, 0), 1)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(_img)\n",
    "\n",
    "\n",
    "def get_rectangles(x_abs, y_abs, w_abs, h_abs, conf, cls):\n",
    "    rectangls = []\n",
    "    batch, anchor, _, H, W = x_abs.shape\n",
    "    for b in range(batch):\n",
    "        recs_batch = []\n",
    "        ind = conf[b] > 0.5\n",
    "        xs = x_abs[b, ind]\n",
    "        ys = y_abs[b, ind]\n",
    "        ws = w_abs[b, ind]\n",
    "        hs = h_abs[b, ind]\n",
    "        confs = conf[b, ind]\n",
    "        clses = cls[b][:, ind[:,0]].argmax(axis=0)\n",
    "        for i in range(len(xs)):\n",
    "            recs_batch.append(\n",
    "                {'x': xs[i], 'y': ys[i], 'w': ws[i], 'h': hs[i],\n",
    "                 'label': int(clses[i]), 'conf':confs[i]})\n",
    "        rectangls += [recs_batch]\n",
    "    return rectangls\n",
    "\n",
    "\n",
    "def nms(recs, iou_thresh=0.5):\n",
    "    nms_output = []\n",
    "    n_recs = len(recs)\n",
    "    rejects = []\n",
    "    for i in range(n_recs):\n",
    "        xi = recs[i]['x']\n",
    "        yi = recs[i]['y']\n",
    "        wi = recs[i]['w']\n",
    "        hi = recs[i]['h']\n",
    "        ci = recs[i]['conf']\n",
    "        for j in range(i+1, n_recs):\n",
    "            if j in rejects:\n",
    "                continue\n",
    "            xj = recs[j]['x']\n",
    "            yj = recs[j]['y']\n",
    "            wj = recs[j]['w']\n",
    "            hj = recs[j]['h']\n",
    "            cj = recs[j]['conf']\n",
    "            if calc_iou((xi, yi, wi, hi), (xj, yj, wj, hj), np) > iou_thresh:\n",
    "                if ci > cj:\n",
    "                    rejects.append(j)\n",
    "                else:\n",
    "                    rejects.append(i)\n",
    "        if i not in rejects:\n",
    "            nms_output.append(recs[i])\n",
    "    return nms_output\n",
    "\n",
    "\n",
    "def demo(img):\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None, ...]\n",
    "    if img.shape[3] == 3:\n",
    "        img = img.transpose(0, 3, 1, 2)\n",
    "    if model.xp == np:\n",
    "        img = cuda.to_cpu(img)\n",
    "    else:\n",
    "        img = cuda.to_gpu(img)\n",
    "    outputs = [cuda.to_cpu(o.data) for o in model.predict(img)]\n",
    "    recs = get_rectangles(*outputs)[0]\n",
    "    recs = nms(recs)\n",
    "    show_rectangls(cuda.to_cpu(img)[0].transpose(1, 2, 0), recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = test_iter.__next__()\n",
    "img = batch[0][0][None,...]\n",
    "t   = batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grand truth\n",
    "show_rectangls(img[0].transpose(1,2,0), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "demo(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
